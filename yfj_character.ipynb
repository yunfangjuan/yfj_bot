{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to install revelant packages below \n",
    "#!pip3 install bs4\n",
    "#!pip3 install openai\n",
    "#!pip3 install python-dotenv\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv \n",
    "import time\n",
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant functions \n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def ask_davinci(question):\n",
    "    response = openai.Completion.create(\n",
    "      model=\"text-davinci-003\",\n",
    "      prompt=question,\n",
    "      temperature=0.1,\n",
    "      max_tokens=2000, \n",
    "      n=1\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "def ask_gpt3(question):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    )\n",
    "    return completion.choices[0].message.content #.strip()\n",
    "\n",
    "def ask_character(post_list, question):\n",
    "    prompt = [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant answering questions based on what the writer YFJ wrote\"\n",
    "        }, \n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here are some articles the writer YFJ wrote:\"\n",
    "        }]\n",
    "   \n",
    "    for post in post_list:\n",
    "        prompt.append({\"role\": \"user\", \"content\": f\"\"\"{post['title']}: {post['content']} \"\"\"})\n",
    "    \n",
    "    prompt.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The writer YFJ was asked the question after the delimiter =====. Please answer the question as if you are the writer YFJ. Start the answer with the word I. Say I don't know or I don't have an opinion if there's no relevant writing from the writer.  ===== {question}\"\n",
    "        })\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        #model= \"gpt-3.5-turbo\",\n",
    "        model= \"gpt-4\",\n",
    "        messages=prompt\n",
    "    )\n",
    "    return completion.choices[0].message.content #.strip()\n",
    "\n",
    "def get_embedding(input): \n",
    "    embedding = openai.Embedding.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=input\n",
    "    )\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute the dot product of vec1 and vec2\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the L2 norms (Euclidean norms) of vec1 and vec2\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate the content database from substack exports\n",
    "\n",
    "post_dir = './greatreset2022_substack' # change this to your dir\n",
    "post_list_csv = f'{post_dir}/posts.csv'\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "with open(post_list_csv, 'r') as file:\n",
    "    post_list = csv.DictReader(file)\n",
    "    for row in post_list:\n",
    "        #print(row)\n",
    "        if row['is_published'] != 'true':\n",
    "            continue\n",
    "        output = {\n",
    "            \"title\": row['title'],\n",
    "            \"post_id\": row['post_id'], \n",
    "            \"subtitle\": row['subtitle']\n",
    "        }\n",
    "        post_file = f'{post_dir}/posts/{row[\"post_id\"]}.html'\n",
    "        with open(post_file, 'r') as f2:\n",
    "            # Read the content of the file\n",
    "            html_string = f2.read()\n",
    "\n",
    "            # Creating a BeautifulSoup object and specifying the parser\n",
    "            soup = BeautifulSoup(html_string, 'html.parser')\n",
    "            \n",
    "            # Finding the title and content\n",
    "            \n",
    "            output['content'] = \" \\n\".join([p.text for p in soup.find_all('p')])\n",
    "            \n",
    "            # Appending the parsed data to the list\n",
    "            parsed_data.append(output)\n",
    "\n",
    "# Converting the list to a JSON string\n",
    "# Open the output file in 'write' mode\n",
    "with open('greatreset.json', 'w') as outfile:\n",
    "    # Write the JSON data to the file\n",
    "    json.dump(parsed_data, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generating Q&A from an existing post \n",
    "articles = []\n",
    "\n",
    "with open('greatreset.json', 'r') as infile:\n",
    "    # Write the JSON data to the file\n",
    "    articles = json.load(infile)\n",
    "\n",
    "\n",
    "qna_list = []\n",
    "max_attempts = 3\n",
    "\n",
    "for post in articles: \n",
    "    post_id = post['post_id']\n",
    "    content = f'{post[\"title\"]} {post[\"subtitle\"]} {post[\"content\"]}'\n",
    "    prompt = f''' generate questions and answers based on the following article delimited by. =====. \n",
    "        Make sure the questions and answers are semantic with the writer's style intact.  \n",
    "        Output the list of questions and answers in the json format where the json represents an array \n",
    "        where each element in the array is a dictionary with keys \"question\" and \"answer\".  \n",
    "        ===== {content} ====='''\n",
    "    print(post[\"title\"])\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Attempt to call the function\n",
    "            answer = ask_gpt3(prompt)\n",
    "            answer_json = json.loads(answer)\n",
    "            # If the function call was successful, break out of the loop\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # If there was an error, print it out and continue to the next attempt\n",
    "            print(f'Attempt {attempt + 1} failed for {post_id} with error: {e}')\n",
    "            time.sleep(5) \n",
    "        \n",
    "    qna_dict = {\n",
    "        \"post_id\": post_id,\n",
    "        \"qa_list\": answer_json\n",
    "    }\n",
    "    qna_list.append(qna_dict)\n",
    "\n",
    "with open('greatreset-processed.json', 'w') as outfile:\n",
    "    # Write the JSON data to the file\n",
    "    json.dump(qna_list, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize the gpt generated output. Ignore if the output is malformed \n",
    "with open('greatreset-processed.json', 'r') as infile:\n",
    "    # Write the JSON data to the file\n",
    "    qna_list = json.load(infile) \n",
    "\n",
    "sanitized_list = []\n",
    "for item in qna_list: \n",
    "    if isinstance(item['qa_list'], list):\n",
    "        sanitized_list.append(item)\n",
    "    else: \n",
    "        #print(item['qa_list'].keys())\n",
    "        keys = list(item['qa_list'].keys()) \n",
    "        if len(keys) == 1: \n",
    "            item['qa_list'] = item['qa_list'][keys[0]]\n",
    "            sanitized_list.append(item)\n",
    "        else:\n",
    "            print(f\"\"\"{item['post_id']} ignored\"\"\"); \n",
    "\n",
    "with open('greatreset-sanitized.json', 'w') as outfile:\n",
    "    # Write the JSON data to the file\n",
    "    qna_list = json.dump(sanitized_list, outfile, indent=4)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding list \n",
    "with open('greatreset-sanitized.json', 'r') as infile:\n",
    "    # Write the JSON data to the file\n",
    "    qna_list = json.load(infile) \n",
    "with open('greatreset.json', 'r') as infile2:\n",
    "    # Write the JSON data to the file\n",
    "    post_list = json.load(infile2) \n",
    "\n",
    "embedding_tuples = [] \n",
    "for item in qna_list: \n",
    "    post_id = item['post_id']\n",
    "    qa_list = item['qa_list']\n",
    "    print(f'''generating embedding for {post_id} from qa ''')\n",
    "    for qa in qa_list: \n",
    "        content = json.dumps(qa) \n",
    "        em = get_embedding(content) \n",
    "        embedding_tuples.append([em, post_id])\n",
    "\n",
    "for post in post_list: \n",
    "    post_id = post['post_id']\n",
    "    print(f'''generating embedding for {post_id} from article ''')\n",
    "    content = f'{post[\"title\"]} {post[\"subtitle\"]} {post[\"content\"]}'\n",
    "    em = get_embedding(content) \n",
    "    embedding_tuples.append([em, post_id])\n",
    "\n",
    "with open('greatreset-embeddings.json', 'w') as outfile:\n",
    "    # Write the JSON data to the file\n",
    "    json.dump(embedding_tuples, outfile, indent=4)      \n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start here if you don't want to retrain the model \n",
    "# Load the embedding data and post database into the memory \n",
    "with open('greatreset-embeddings.json', 'r') as infile:\n",
    "    # Write the JSON data to the file\n",
    "    embedding_tuples = json.load(infile) \n",
    "\n",
    "print(len(embedding_tuples))\n",
    "with open('greatreset.json', 'r') as infile2:\n",
    "    # Write the JSON data to the file\n",
    "    post_list = json.load(infile2) \n",
    "print(len(post_list))\n",
    "\n",
    "post2content = {}\n",
    "for post in post_list: \n",
    "    post2content[post['post_id']] = post \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ask a question, get an answer  \n",
    "\n",
    "#question = \"What do you think of Tesla's stock price?\" \n",
    "#question = \"What do you think of Meta's Metaverse?\"\n",
    "#question = \"Do you think the Fed does a good job?\"\n",
    "#question = \"What are your stock picks for 2023?\"\n",
    "#question = \"Do you think there will be another great depression?\"\n",
    "#question = \"Will there be IPOs in 2023?\"\n",
    "#question = \"What will happen to venture capital in 2023?\"\n",
    "#question = \"Do you think the Fed will lower interest rates in 2023?\"\n",
    "#question = \"Do you think AI is a danger to humanity?\"\n",
    "#question = \"Do you think there will be more banks going out of business?\"\n",
    "#question = \"What do you think of tether?\" \n",
    "question = \"What do you think of the 2028 LA olympics?\" # plug in your question here \n",
    "emq = get_embedding(question) \n",
    "\n",
    "scores = []\n",
    "post_ids = []\n",
    "\n",
    "for et in  embedding_tuples: \n",
    "    post_ids.append(et[1])\n",
    "    scores.append(cosine_similarity(emq, et[0]))\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "N=10\n",
    "scores_array = np.array(scores)\n",
    "\n",
    "# Get the indices that would sort the array\n",
    "sorted_indices = np.argsort(scores_array)\n",
    "\n",
    "# Get the top N indices. Note: [::-1] is used to reverse the array because argsort sorts in ascending order\n",
    "top_N_indices = sorted_indices[-N:]\n",
    "scores_array[top_N_indices]\n",
    "\n",
    "print(top_N_indices)\n",
    "print(scores_array[top_N_indices])\n",
    "\n",
    "relevant_posts = list(set(np.array(post_ids)[top_N_indices]))\n",
    "print(relevant_posts)\n",
    "\n",
    "input_list = []\n",
    "for p in relevant_posts: \n",
    "    input_list.append(post2content[p])\n",
    "result = ask_character(input_list, question)\n",
    "\n",
    "print(\"=======================\")\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"YFJ Bot: {result}\") \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
